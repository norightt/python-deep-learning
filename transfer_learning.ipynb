{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/norightt/python-deep-learning/blob/main/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBqj0gIiUcEF"
      },
      "source": [
        "# Домашнее задание 3. Детекция объектов\n",
        "\n",
        "Сыграем в квиддич? Или лучше в карты?\n",
        "\n",
        "В этом дз вам предстоит написать практически с нуля архитектуру для детекции, а также воспользоваться готовым решением. На выбор даётся два датасета, отличаются они только картинками. Форматы, баллы - все одинаково.\n",
        "\n",
        "Первый вариант это датасет по кадрам игры в квиддич из Гарри Поттера. Если вы забыли правила, то нажмите [сюда](https://harrypotter.fandom.com/ru/wiki/%D0%9A%D0%B2%D0%B8%D0%B4%D0%B4%D0%B8%D1%87). Вы научитесь искать и выделять на фотографиях бладжеры, квоффл и снитч.\n",
        "\n",
        "Второй вариант это датасет с игральными картами. Если вы забыли что такое карты, то нажмите [сюда](https://ru.wikipedia.org/wiki/%D0%98%D0%B3%D1%80%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5_%D0%BA%D0%B0%D1%80%D1%82%D1%8B). Вы научитесь искать и выделять на фотографиях несколько типов карт.\n",
        "\n",
        "Оба варианта содержат около 300 картинок, данные хранятся в xml в формате PascalVOC. Есть малые отличия, но ничего страшного.\n",
        "\n",
        "\n",
        "Если с самописным детектором совсем не получается, то можно после создания датасетов перейти к концу, где обучается готовый, с ним будет проще :)\n",
        "\n",
        "### А ещё есть консультация 2023 года :)\n",
        "\n",
        "https://www.youtube.com/watch?v=oCwEmKW3bYg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6Mkra5AA0rV"
      },
      "source": [
        "# Notes\n",
        "\n",
        "Дз проверялось на работоспособность в colab. Не гарантируется, что будет работать на чем-то другом. На Windows, скорее всего, вас ждет немало проблем :(\n",
        "\n",
        "По вопросам формулировок (не ошибок торча!), в случае отсутствия ответа в общем чате (поиск по чату позволяет проверить), можно написать в него с тегом @markblumenau.\n",
        "\n",
        "Отдельная благодарность А. Абрамову за написание assert для проверки функций и В. Гилязову за поясняющую картинку."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8bl6rVf-F_1"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Скачайте один из датасетов на свой вкус и начните работу с ним.\n",
        "Разметка находится в xmls папке, картинки в images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ByC-_g-Kdn",
        "outputId": "90d29b27-81ba-4768-a972-06706625f493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-14 13:31:39--  https://github.com/markblumenau/hw3_iad_dl/raw/main/cards/data.zip\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/markblumenau/hw3_iad_dl/main/cards/data.zip [following]\n",
            "--2024-11-14 13:31:40--  https://raw.githubusercontent.com/markblumenau/hw3_iad_dl/main/cards/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38392108 (37M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  36.61M  47.5MB/s    in 0.8s    \n",
            "\n",
            "2024-11-14 13:31:41 (47.5 MB/s) - ‘data.zip’ saved [38392108/38392108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Cards\n",
        "!wget https://github.com/markblumenau/hw3_iad_dl/raw/main/cards/data.zip\n",
        "\n",
        "\n",
        "\n",
        "!unzip -q data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkroFQHZXZwQ"
      },
      "source": [
        "# Задача 1. 0.5 балла.\n",
        "\n",
        "Ниже написан код для стандартного Dataset из библиотеки pytorch. Dataset требует реализации `__getitem__` и `__len__` методов. Далее эти методы будут использованы для формирования батчей для обучения. Поскольку читать придется из xml файлов, нужно перед этим дописать функцию get_xml_data, чтобы по названию картинки подтягивать аннотации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-v0_fX4A0rW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446379a9-68c8-4f3e-ccea-c25e9e8dfe5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from xml.etree import ElementTree as ET\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import nn\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxtRgBlowu8q"
      },
      "source": [
        "Функции можно и нужно передать некий class_dict. Он есть и при инициализации датасета ниже. С его помощью можно название класса превратить в int. Далее подразумевается, что класс идёт как int."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_xml_data(image_name, root, class_dict, xml_prefix=\"/xmls/\"):\n",
        "    # имя файла без расширения\n",
        "    filename = image_name.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    # чтение xml файла\n",
        "    tree = ET.parse(str(root) + xml_prefix + filename + \".xml\")\n",
        "    treeroot = tree.getroot()\n",
        "\n",
        "    bboxes = []\n",
        "    for member in treeroot.findall(\"object\"):\n",
        "        # достаем координаты bbox\n",
        "        # https://stackoverflow.com/questions/65673622/bounding-box-extraction\n",
        "        # https://github.com/douglasrizzo/detection_util_scripts/issues/10\n",
        "        xmin = int(member.find(\"bndbox/xmin\").text)\n",
        "        ymin = int(member.find(\"bndbox/ymin\").text)\n",
        "        xmax = int(member.find(\"bndbox/xmax\").text)\n",
        "        ymax = int(member.find(\"bndbox/ymax\").text)\n",
        "\n",
        "        # сопоставляем имя класса с его индексом\n",
        "        class_name = member.find(\"name\").text\n",
        "        class_id = class_dict[class_name]\n",
        "\n",
        "\n",
        "        res = [xmin, ymin, xmax, ymax, class_id]\n",
        "        bboxes.append(res)\n",
        "\n",
        "    return bboxes\n"
      ],
      "metadata": {
        "id": "ZKKclMMF6_k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqnUStDzgGnS"
      },
      "outputs": [],
      "source": [
        "class PascalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, *, transform, root=\"dataset\", train=True, seed=42):\n",
        "        self.root = Path(root)\n",
        "        self.transform = transform\n",
        "\n",
        "        assert self.root.is_dir(), f\"No data at `{root}`\"\n",
        "\n",
        "        self.filenames = np.array(glob.glob(root + \"/images/*\"))\n",
        "        with open(str(self.root) + \"/class_dict\", \"r\") as f:\n",
        "            self.class_dict = eval(f.readline())\n",
        "\n",
        "        self.class_dict_inv = {v: k for k, v in self.class_dict.items()}\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        permutation = np.random.permutation(len(self.filenames))\n",
        "\n",
        "        # Train/test split\n",
        "        if train:\n",
        "            self.filenames = self.filenames[\n",
        "                permutation[: int(len(self.filenames) * 0.9)]\n",
        "            ].tolist()\n",
        "        else:\n",
        "            self.filenames = self.filenames[\n",
        "                permutation[int(len(self.filenames) * 0.9) :]\n",
        "            ].tolist()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Нужно обладая файлнеймом подгрузить картинку и бибоксы, функцию для подгрузки бибоксов вы дописали выше :)\n",
        "        fname = self.filenames[idx]\n",
        "        image = ### YOUR CODE HERE ###\n",
        "        bboxes = ### YOUR CODE HERE ###\n",
        "\n",
        "        # transform будут объявлены ниже за вас\n",
        "        # Но библиотека привередлива к формату данных на входе, внимательно прочтите прошлую клеточку\n",
        "        return self.transform(image=image, bboxes=bboxes)\n",
        "\n",
        "    def __get_raw_item__(self, idx):\n",
        "        fname = self.filenames[idx]\n",
        "        return fname, get_xml_data(fname, self.root, self.class_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        ### YOUR CODE HERE ###\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upuJ9g4SXwtx"
      },
      "source": [
        "Ниже определяем стандартные нормализации и приведение размера к 512x512.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj5DYhiknhAp"
      },
      "outputs": [],
      "source": [
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(512, 512),\n",
        "        A.augmentations.transforms.Normalize(mean=mean, std=std),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    # Вот тут мы говорим что хотим pascal_voc\n",
        "    bbox_params=dict(format=\"pascal_voc\", min_visibility=0.3),\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(512, 512),\n",
        "        A.augmentations.transforms.Normalize(mean=mean, std=std),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=dict(format=\"pascal_voc\", min_visibility=0.5),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHhV_8UXEsIB"
      },
      "outputs": [],
      "source": [
        "train_ds = PascalDataset(root=\"./data/\", transform=train_transform, train=True)\n",
        "test_ds = PascalDataset(root=\"./data/\", transform=test_transform, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhWaKC0NTYPL"
      },
      "source": [
        "# Задача 2. 1 балл.\n",
        "\n",
        "Теперь, когда мы загрузили данные, хорошо бы посмотреть на них, прежде чем обучать какие-либо модели. Напишите функцию `visualize`, которая принимает списки изображений и прямоугольников в качестве входных данных и рисует эти прямоугольники на изображениях.\n",
        "\n",
        "В датасете есть class_dict_inv, который позволит вам сделать обратное преобразование: int, содержащий класс, в строку с названием.\n",
        "\n",
        "\n",
        "Полезные функции:\n",
        "* [plt.subplots](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html) -- легко создавать несколько изображений в одной pyplot figure\n",
        "* [ax.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.imshow.html) -- отображение графиков (не забудьте откатить нормализацию)\n",
        "* [ax.text](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html), [patches.Rectangle](https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html) -- для рисования прямоугольников и текста с аннотацией"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMlang0KJHS7"
      },
      "outputs": [],
      "source": [
        "def visualize(images, bboxes):\n",
        "    mean = (0.485, 0.456, 0.406)\n",
        "    std = (0.229, 0.224, 0.225)\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        2, len(images) // 2 + len(images) % 2, figsize=(10, 8), dpi=100\n",
        "    )\n",
        "\n",
        "    for i, ax in enumerate(axes.reshape(-1)):\n",
        "\n",
        "        ax.axis(False)\n",
        "\n",
        "        if i >= len(images):\n",
        "            break\n",
        "        # Вот тут нужно выполнить permute (вспомните где у torch каналы, а где они у matplotlib)\n",
        "        # И откатить нормализацию (просто обратное преобразование)\n",
        "        # Имена и количества классов можно подтянуть из датасета через train_ds.class_dict_inv\n",
        "\n",
        "        # Вот тут покажите картинку после отката нормализации\n",
        "        ax.imshow(### YOUR CODE HERE ###)\n",
        "\n",
        "        for bbox in bboxes[i]:\n",
        "        # Вот тут нарисуйте бибоксы\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oHM4iPKTPzk"
      },
      "source": [
        "У вас должно получиться что-то похожее на изображения для датасета с масками:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WSPRNrKA0rY"
      },
      "source": [
        "![image](https://i.imgur.com/V5TUT26.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuFWYK8sA0rZ"
      },
      "outputs": [],
      "source": [
        "out = [train_ds[i] for i in range(6)]\n",
        "visualize([o[\"image\"] for o in out], [o[\"bboxes\"] for o in out])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReVJR_kYVTfO"
      },
      "source": [
        "# Задача 3. 3 балла.\n",
        "## YOLO-like детектор\n",
        "\n",
        "Сейчас нам предстоить реализовать детектор, похожий на YOLO. Это один из самых простых детекторов с точки зрения реализации. YOLO описан в статье: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640). Здесь мы его немного изменим и упростим. Будем использовать ResNet для извлечения признаков. На выходе мы будем получать карту признаков размера 16x16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3738iQSA0ra"
      },
      "source": [
        "## Задача 3.1. 1 балл.\n",
        "\n",
        "Первым делом нам нужно реализовать collate function. Это функция позволит нам кастомизировать, как именно батч конструируется из примеров (смотрите [pytorch docs](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) для деталей).\n",
        "\n",
        "Это функция должна принять на вход лист прямоугольников и вернуть тензор размера Bx6x16x16. Первая размерность - это количество примеров в батче. Вторая -- \"каналы\", суть которых написана чуть ниже. Далее идут две пространственные размерности, это сетка 16 на 16. Зафиксируем порядок координат как (y, x) для этой сетки 16 на 16. Это значит, что в target идут ```cy_idx, cx_idx``` в таком порядке, см. код.\n",
        "\n",
        "В шести \"каналах\" у нас будут записаны:\n",
        "* Сдвиги центра bbox относительно начала клеточки (клеточка это \"гиперпиксель\" на изображении 16 на 16 на выходе сети). Записаны эти сдвиги будут в клеточку, к которой относятся. 2 канала (X, Y)\n",
        "* Нормализованные ширина и высота bbox. 2 канала (W, H)\n",
        "* Confidence сетки. Им мы будем пользоваться, чтобы фильтровать уверенность сетки в наличии bbox в данной клетке. Таргет содержит 1 там, где bbox есть, и 0 иначе. 1 канал\n",
        "* Класс детекции (тот самый int, полученный из строки с названием)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbkJdK52wu8r"
      },
      "source": [
        "### Пояснительная картинка.\n",
        "\n",
        "Она обладает некоторым уровнем абстракции, чёрных например должно быть 512, а зелёная должна захватывать 32 маленькие черные. Но может будет понятнее :)\n",
        "\n",
        "![image](https://i.imgur.com/13YVxAd.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzOTozOVKzt1"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, downsample=32):\n",
        "    imgs, batch_boxes = map(list, (zip(*[(b[\"image\"], b[\"bboxes\"]) for b in batch])))\n",
        "\n",
        "    imgs = torch.stack(imgs)\n",
        "    b, _, h, w = imgs.shape\n",
        "\n",
        "    target = imgs.new_zeros(b, 6, h // downsample, w // downsample)\n",
        "\n",
        "    # Add sample index to targets\n",
        "    for i, boxes in enumerate(batch_boxes):\n",
        "        xmin, ymin, xmax, ymax, classes = map(\n",
        "            torch.squeeze, torch.split(imgs.new_tensor(boxes), 1, dim=-1)\n",
        "        )\n",
        "\n",
        "        # Нормализуйте ширину и высоту, поделив на ширину и высоту исходного изображения\n",
        "        x_cell =  # TODO размер клетки по X в пикс\n",
        "        y_cell =  # TODO размер клетки по Y в пикс\n",
        "\n",
        "        # ширина и высота бибокса могут превышать размеры гиперпикселя\n",
        "        # поэтому их мы нормируем на полноценные 512\n",
        "        w_box =  # TODO ширина бокса отнормированная на размер изначальной картинки\n",
        "        h_box =  # TODO высота бокса отнормированная на размер изначальной картинки\n",
        "\n",
        "        # Посчитайте координаты центра и сдвиги\n",
        "        cx =  # TODO (координаты центра в исходных координатах)\n",
        "        cy =  # TODO\n",
        "        cx_idx =  # TODO (посчитайте индекс центра на карте признаков размера 16x16. Это будут как раз координаты пикселя, куда мы запишем параметры коробки)\n",
        "        cy_idx =  # TODO\n",
        "\n",
        "        cx_box =  # TODO (посчитайте сдивиги относительно cx_idx)\n",
        "        cy_box =  # TODO\n",
        "\n",
        "        # cy_idx, потом cx_idx\n",
        "        target[i, :, cy_idx, cx_idx] = torch.stack(\n",
        "            [cx_box, cy_box, w_box, h_box, torch.ones_like(cx_box), classes]\n",
        "        )\n",
        "\n",
        "    return {\"image\": imgs, \"target\": target}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC01qB-ywu8s"
      },
      "source": [
        "Следующей функцией Вы можете проверить свою реализацию. Проверка не является блокирующей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb-s5F1Wwu8s"
      },
      "outputs": [],
      "source": [
        "def test_collate_fn() -> None:\n",
        "    target1 = [ 100, 200, 200, 300, 2 ] # xmin, ymin, xmax, ymax, class\n",
        "    target2_1 = [ 0, 250, 200, 300, 0 ] # xmin, ymin, xmax, ymax, class\n",
        "    target2_2 = [ 0, 100, 100, 300, 1 ] # xmin, ymin, xmax, ymax, class\n",
        "\n",
        "    result = collate_fn([\n",
        "        { 'image': torch.rand((3, 512, 512)), 'bboxes': [ target1 ] },\n",
        "        { 'image': torch.rand((3, 512, 512)), 'bboxes': [ target2_1, target2_2 ] }\n",
        "    ])\n",
        "\n",
        "    # Проверяем размерности\n",
        "    assert result['image'].shape == (2, 3, 512, 512)\n",
        "    assert result['target'].shape == (2, 6, 16, 16)\n",
        "\n",
        "    # Проверяем значения клеточек, в которые попали ббоксы\n",
        "    assert np.allclose(result['target'][0, :, 7, 4], torch.tensor([ 22 / 32, 26 / 32, 100 / 512, 100 / 512, 1, 2 ]))\n",
        "    assert np.allclose(result['target'][1, :, 8, 3], torch.tensor([ 4 / 32, 19 / 32, 200 / 512, 50 / 512, 1, 0 ]))\n",
        "    assert np.allclose(result['target'][1, :, 6, 1], torch.tensor([ 18 / 32, 8 / 32, 100 / 512, 200 / 512, 1, 1 ]))\n",
        "\n",
        "    # Проверяем, что все остальные клеточки содержат нули\n",
        "    result['target'][0, :, 7, 4] = result['target'][1, :, 8, 3] = result['target'][1, :, 6, 1] = torch.zeros(6)\n",
        "    assert np.allclose(result['target'], 0)\n",
        "\n",
        "test_collate_fn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqpwb_7lU3_u"
      },
      "source": [
        "Ниже вы можете увидеть пример, как выглядит решетка размера 16 на 16 на исходном изображении:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWXgBxyrU-mX"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "i = 20\n",
        "\n",
        "img = train_ds[i][\"image\"].permute(1, 2, 0) * torch.tensor(std).view(\n",
        "    1, 1, -1\n",
        ") + torch.tensor(mean).view(1, 1, -1)\n",
        "bboxes = torch.tensor(train_ds[i][\"bboxes\"])\n",
        "\n",
        "ax.imshow(img)\n",
        "loc = plt.matplotlib.ticker.MultipleLocator(base=32)\n",
        "ax.xaxis.set_major_locator(loc)\n",
        "ax.yaxis.set_major_locator(loc)\n",
        "ax.grid(which=\"major\", axis=\"both\", linestyle=\"-\", linewidth=3)\n",
        "\n",
        "for bbox in bboxes:\n",
        "    xmin, ymin, xmax, ymax = bbox[:-1]\n",
        "    w = xmax - xmin\n",
        "    h = ymax - ymin\n",
        "    with_mask = bbox[-1]\n",
        "    ax.add_patch(Rectangle((xmin, ymin), w, h, fill=False, color=\"red\"))\n",
        "\n",
        "cx = (bboxes[:, 0] + bboxes[:, 2]) / 2\n",
        "cy = (bboxes[:, 1] + bboxes[:, 3]) / 2\n",
        "\n",
        "ax.scatter(cx, cy, color=\"green\", marker=\"o\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6ddzkGdUp54"
      },
      "source": [
        "## Задача 3.2. 0.5 балла.\n",
        "\n",
        "Выход нашей сетки будет несколько больше, чем Bx6x16x16. Почему?\n",
        "\n",
        "Мы решаем задачу, где классов больше одного. Вспомним прошлое дз: target был одним числом, но выход сетки содержал длинный-длинный вектор, из которого мы получали вероятность принадлежности к тому или иному классу. Здесь то же самое, но как бы в двумерии: у каждой клеточки из этих 16*16 будет свой вектор длины C, который мы будем использовать для определения класса.\n",
        "\n",
        "Реализуйте обратное относительно collate_fn преобразования, чтобы декодировать выход нейронной сети. Применив функцию decode_prediction к выходу collate function вы должны получить изначальный набор прямоугольников с корректными размерами и координатами, а также классами. Применив к выходу нейросети мы тоже должны получить набор прямоугольников и тоже с корректными классами.\n",
        "\n",
        "То есть, нужно проделать операции из collate_fn в обратную сторону, но учесть, что у неройнки выход будет чуть длиннее, и там мы должны брать argmax для определения класса.\n",
        "\n",
        "Hint: в target classes идут в конце. В нейронке они тоже будут в конце, но их будет больше 1. Можно проверять число каналов пришедшего объекта, если оно 6, то перед нами target и надо брать значение, которое записано в клеточке. Иначе (каналов больше 6) перед нами выход нейронки, и надо брать самый вероятный из них."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyxuODIqzpLf"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(pred, upsample=32, threshold=0.7):\n",
        "    b, c, h, w = pred.shape\n",
        "    img_w, img_h = w * upsample, h * upsample\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4dL4Kgbwu8s"
      },
      "source": [
        "Следующей функцией Вы можете проверить свою реализацию. Проверка не является блокирующей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otG5s2ABwu8s"
      },
      "outputs": [],
      "source": [
        "def test_decode_predictions() -> None:\n",
        "    # Применяем collate_fn к некоторым данным\n",
        "    target1 = [ [ 100, 200, 200, 300, 2 ] ]\n",
        "    target2 = [ [ 0, 250, 200, 300, 0 ], [ 0, 100, 100, 300, 1 ] ]\n",
        "    result = collate_fn([\n",
        "        { 'image': torch.rand((3, 512, 512)), 'bboxes': target1 },\n",
        "        { 'image': torch.rand((3, 512, 512)), 'bboxes': target2 }\n",
        "    ])\n",
        "\n",
        "    target = result['target']\n",
        "    assert target.shape == (2, 6, 16, 16)\n",
        "    # В таком виде target соответствует поиску одного класса - 6-ой канал.\n",
        "    # Добавим еще 2 канала, чтобы проверить работу функции для нескольких классов\n",
        "    target = torch.cat([ target, torch.zeros(2, 2, 16, 16) ], dim = 1) # Добавляем два канала\n",
        "    assert target.shape == (2, 8, 16, 16) # Теперь их стало 8\n",
        "\n",
        "    target[0, 5:, 7, 4] = torch.tensor([ 0.5, 0.6, 0.9 ]) # Это должен быть класс 2\n",
        "    target[1, 5:, 8, 3] = torch.tensor([ 0.9, 0.6, 0.5 ]) # Это должен быть класс 0\n",
        "    target[1, 5:, 6, 1] = torch.tensor([ 0.6, 0.9, 0.5 ]) # Это должен быть класс 1\n",
        "\n",
        "    # Считаем результат\n",
        "    actual = decode_prediction(target)\n",
        "    print('Actual:  ', actual)\n",
        "\n",
        "    # Порядок ббоксов для target2 может меняться в зависимости от вашей реализации и это не ошибка.\n",
        "    # Скорее всего, он будет такой. Но если ассерт не проходит, попробуйте поменять их местами.\n",
        "    expected = [ target1, [ target2[1], target2[0] ] ]\n",
        "    # expected = [ target1, [ target2[0], target2[1] ] ] # <-- такой ответ тоже правильный.\n",
        "    print('Expected:', expected)\n",
        "\n",
        "    # Сравниваем\n",
        "    assert actual == expected\n",
        "\n",
        "test_decode_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB7Jy8hEVlah"
      },
      "source": [
        "## Задача 3.3. 1 балл.\n",
        "Реализуйте модель. Первым делом примените первые 4 блока (до layer4 включительно) ResNet50. Далее добавьте несколько блоков (Conv2D, BatchNorm2D, ReLU). Постепенно уменьшайте количество каналов до 5+C, а размер изображения до 16 на 16. Например, 2048 -> 512 -> 128 -> 32 -> 5+C, где С - количество классов в вашем датасете. Размер ядра при этом 3, паддинг 1. Но вариантов много, попробуйте разные! **Последним слоем обязательно должна быть свертка.** Так как все значения, которые мы предсказываем, находятся в отрезке от 0 до 1 (благодаря нормировке с клеточками), мы после финальной свертки еще применим сигмоиду. Для классов в такой постановке это не навредит.\n",
        "\n",
        "Если будете фантазировать, то для получения правильного размера изображения после сети не стесняйтесь применять слои с фильтрами больше 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWnJu9Yl7m3o"
      },
      "outputs": [],
      "source": [
        "C =  # Количество классов в вашем датасете, хоть руками посчитайте, хоть подтяните из словаря классов\n",
        "\n",
        "\n",
        "class Detector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, img):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-CrPCBlV1Iw"
      },
      "source": [
        "## Задача 3.4. 0.5 балла.\n",
        "\n",
        "Реализуйте функцию потерь.\n",
        "\n",
        "Для этого:\n",
        "* Сделайте маску, которая будет говорить о положении детектируемых объектов. Её нужно использовать с помощью masked_select (см. доки PyTorch)\n",
        "* Лосс похож на оригинальный для Yolo V1 и состоит из 4 частей (reduction='sum' для всех)\n",
        "    - localization loss - Мы берем MSE по координатам бокса там, где есть детектируемый объект\n",
        "    - box_loss - MSE от корней ширины и высоты bbox там, где есть детектируемый объект\n",
        "    - classification_loss - Если детектируемый объект есть, то его кросс-энтропия по его классу\n",
        "    - confidence_loss - Бинарная кросс-энтропия факта наличия объекта ДЛЯ ВСЕХ пикселей. Делается отдельно для детектируемых объектов (вес 1) и для недетектируемых (вес 0.1 например, поскольку их гораздо больше, но можно экспериментировать)\n",
        "\n",
        "\n",
        "* Если будете делать описанное выше, то учтите reduction. Бинарная кросс-энтропия вызывается через BCELoss. Параметр C используется для задачи числа классов. Подумайте как зависит индексация от параметра C и используйте его."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTeFFazVCKbm"
      },
      "outputs": [],
      "source": [
        "def special_loss(pred, target, C=C):\n",
        "    ### YOUR CODE HERE ###\n",
        "    localization_loss =\n",
        "    box_loss =\n",
        "    classification_loss =\n",
        "    confidence_loss =\n",
        "\n",
        "    return localization_loss + box_loss + classification_loss + confidence_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE5yID_-XUEv"
      },
      "source": [
        "# Задача 4. 2 балла.\n",
        "\n",
        "Обучите вашу модель (написав цикл обучения), и покажите что она работает (скорее всего, объекты найдутся на 1-2 картинках)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39dMMAu49odu"
      },
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(train_ds, 10, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQiDhPZlN7OR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(21)\n",
        "EPOCHS =  # Harry Potter 20, Cards 15\n",
        "model = Detector().to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for e in tqdm(range(EPOCHS)):\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    epoch_losses = []\n",
        "    for batch in pbar:\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "    print(f\"Epoch {e} done; Train loss {np.mean(epoch_losses):.3f};\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocp2M0-zWiUT"
      },
      "source": [
        "Запустим обученный детектор на тестовых изображениях:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukgmauCoEWWk"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_ds, 6, collate_fn=collate_fn)\n",
        "i = iter(test_loader)\n",
        "batch = next(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7oY4PtfPn1T"
      },
      "outputs": [],
      "source": [
        "# Нужно сделать предсказание и переложить результат на cpu\n",
        "\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBdWfhzt0Slq"
      },
      "outputs": [],
      "source": [
        "# Сделайте визуализацию. Поиграйтесь с threshold, скорее всего нужно понизить до ~0.1\n",
        "\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TbzYExuuqO1"
      },
      "source": [
        "Результат сильно так себе, да? Есть множество вариантов улучшений, самый простой из которых это приделать к выходу [NMS](https://paperswithcode.com/method/non-maximum-suppression#:~:text=Non%20Maximum%20Suppression%20is%20a,below%20a%20given%20probability%20bound.). Если хочется, можно почитать про YOLO v1 [тут](https://arxiv.org/abs/1506.02640)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbKKfFh4C9yV"
      },
      "source": [
        "# Задача 5. 3.5 балла.\n",
        "\n",
        "Займёмся более простыми вещами. Возьмем готовую архитектуру, обучим её на наших данных и посмотрим.\n",
        "\n",
        "Для этого будем использовать YOLO 11 от ultralytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXazCJnA0rd"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78dNxuJsA0rd"
      },
      "source": [
        "## Задача 5.1. 1.5 балла.\n",
        "\n",
        "Чтобы дальше модель обучалась одной строкой, данные нужно переложить в правильный формат. Да-да, классика перекладывания JSON. Как правильно паковать можно посмотреть [тут](https://roboflow.com/formats/yolov8-pytorch-txt).\n",
        "\n",
        "Если коротко:\n",
        "* Есть .yaml, где живут пути к папкам с картинками, количество классов и их названия\n",
        "* Есть папочки train valid (их поможем вам собрать), в них две подпапки:\n",
        "    - Первая images, в ней лежат картинки\n",
        "    - Вторая labels, в ней лежат файлы с названиями как у картинок, но вместо расширения картинок нужен .txt, внутри формат как описан на Roboflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHSn1qlyA0rd"
      },
      "outputs": [],
      "source": [
        "# Делаем папочки\n",
        "!rm -rf train\n",
        "!rm -rf valid\n",
        "!mkdir -p train/images train/labels valid/images valid/labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rShRY2NtA0rn"
      },
      "source": [
        "Реализуйте функцию, которая принимает аннотации в изначальном формате, а возвращает их в нужном для YOLO 11. Это должен быть массив готовых строк, которые можно сразу забрасывать в файлик, добавив \\n.\n",
        "\n",
        "Использовать сторонние инструменты нельзя, нужно переложить своими руками."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70rHtPORA0rn"
      },
      "outputs": [],
      "source": [
        "def annotation2txt(bboxes, w_im, h_im):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47EInQx-A0ro"
      },
      "outputs": [],
      "source": [
        "# Копируем картиночки по папочкам и создаем txt файлики\n",
        "\n",
        "for i in range(len(train_ds)):\n",
        "    result = train_ds.__get_raw_item__(i)\n",
        "\n",
        "    shutil.copyfile(\n",
        "        result[0], \"./train/images/\" + result[0].split(\"/\")[-1],\n",
        "    )\n",
        "\n",
        "    h_im, w_im, ch = np.array(Image.open(result[0])).shape\n",
        "    with open(\n",
        "        \"./train/labels/\" + result[0].split(\"/\")[-1].split(\".\")[0] + \".txt\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        f.write(\"\\n\".join(annotation2txt(result[1], w_im, h_im)))\n",
        "\n",
        "for i in range(len(test_ds)):\n",
        "    result = test_ds.__get_raw_item__(i)\n",
        "\n",
        "    shutil.copyfile(\n",
        "        result[0], \"./valid/images/\" + result[0].split(\"/\")[-1],\n",
        "    )\n",
        "\n",
        "    with open(\n",
        "        \"./valid/labels/\" + result[0].split(\"/\")[-1].split(\".\")[0] + \".txt\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        f.write(\"\\n\".join(annotation2txt(result[1], w_im, h_im)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rziJVW6A0ro"
      },
      "outputs": [],
      "source": [
        "# Собираем YAML\n",
        "\n",
        "nc =  # Укажите число классов. Хоть руками, хоть по-умному посчитайте (см. class_dict)\n",
        "names =  # Укажите имена классов. Хоть руками, хоть по-умному посчитайте, это массив строк (см. class_dict)\n",
        "\n",
        "with open(\"data.yaml\", \"w\") as f:\n",
        "    f.write(f\"train: ../train/images\\nval: ../valid/images\\n\\nnc: {nc}\\nnames: {names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG3YkSNkA0ro"
      },
      "source": [
        "## Задание 5.2. 1.5 балла.\n",
        "\n",
        "Обучите модель YOLO 11 самого маленького размера. Библиотека максимально friendly, от вас требуется написать две строчки. Модель можно взять обученную.\n",
        "\n",
        "Подсказка: подумайте зачем вам data.yaml и что такое yolo11n.yaml (не стесняйтесь гуглить)\n",
        "\n",
        "Если у вас лосс NaN или Windows, то есть несколько полезных ссылок (покрывают не все существующие проблемы, но может поможет вам):\n",
        "\n",
        "https://github.com/ultralytics/ultralytics/issues/1149\n",
        "\n",
        "https://github.com/ultralytics/yolov5/issues/6907\n",
        "\n",
        "https://stackoverflow.com/questions/75178762/i-got-nan-for-all-losses-while-training-yolov8-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ke0n7wiA0ro"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4H31vNkA0ro"
      },
      "source": [
        "## Задание 5.3. 0.5 балла.\n",
        "\n",
        "Как-нибудь отрисуйте предсказания на валидационной выборке (хотя бы части из 5-10 картинок).\n",
        "\n",
        "Здесь можно использовать костыли с параметром save=True у predict, потом прочитать их чем-нибудь, отрисовать матплотлибом. Есть варианты и получше. Дефолтный show будет пытаться показывать через opencv imshow, он в коллабе работать не будет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjeV-C-LA0rp"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "type_ipynb_hw": "task"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}